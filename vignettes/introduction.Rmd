---
title: "janitor overview"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(collapse = T, comment = "#>")
library(janitor)
```

>Data scientists, according to interviews and expert estimates, spend from 50 percent to 80 percent of their time mired in this more mundane labor of collecting and preparing unruly digital data, before it can be explored for useful nuggets.
>
> -- *"[For Big-Data Scientists, 'Janitor Work' Is Key Hurdle to Insight](http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html)" - The New York Times, 2014*

The janitor package has functions to expedite the initial data exploration and cleaning that comes with any new data set.

### Examining functions
* `tabyl()` - an enhanced replacement for `table()`
* `crosstab()`
* `get_dupes()`
* `top_levels()`

### Cleaning functions
* `clean_names()`
* `clean_NA_variants()` and `clean_NA_vec()`
* `excel_numeric_to_date()`
* `remove_empty_cols()` and `remove_empty_rows()`

# Cleaning with janitor
## Clean data.frame names with `clean_names()`
Call this function every time you read data.

It works in a `%>%` pipeline, and handles the problematic variable names that are so well preserved by `readxl::read_excel()` and `readr::read_csv()`.

+ Returns names with only lowercase letters, with `_` as a separator
+ Handles special characters and spaces
+ Appends numbers to duplicated names
+ Converts "%" to "percent" to retain meaning

```{r, message = FALSE}
# Load dplyr for the %>% pipe
library(dplyr)
# Create a data.frame with dirty names
test_df <- data.frame(matrix(ncol = 6) %>% as.data.frame())
names(test_df) <- c("two words", "repeat value", "REPEAT VALUE", "% successful (2009)",  "abc@!*", "")

clean_df <- test_df %>% clean_names()
names(clean_df) # they are clean
```


# Examining with janitor
## `tabyl()` - a better version of `table()`
`tabyl()` takes a vector and returns a frequency table, like `table()`. But its additional features are:

+ It returns a data.frame (actually, a `tbl_df`) - for sending to `ggplot()` or `kable()`, or manipulating further
+ It automatically calculates percentages
+ It can (optionally) display `NA` values
    + When `NA` values are present, it will calculate an additional column `valid_percent` in the style of SPSS
+ It can (optionally) sort on counts

```{r}
x <- c("a", "b", "c", "c", NA)
tabyl(x)
# compare to:
table(x)
```

## Crosstabulate two variables with `crosstab()`
`crosstab()` generates a crosstab table.  There many crosstab functions already; this one is distinguished by:
+ It returns a data.frame (actually, a `tbl_df`)
+ It is simple.
    + It calculates frequencies by default but can calculate row, column, and table-wise percentages.
    + It can (optionally) display `NA` values

It wraps the common pipeline of `group_by %>% summarise %>% mutate %>% spread` from the dplyr and tidyr packages, often used in exploratory analysis.

```{r}
y <- c(1, 1, 2, 1, 2)
x <- c("a", "a", "b", "b", NA)

crosstab(x, y)
crosstab(x, y, percent = "row")
```
This gives the same result as the much longer pipeline:
```{r, message=FALSE, results = "hide"}
library(dplyr) ; library(tidyr)
data_frame(x, y) %>%
  group_by(x, y) %>%
  tally() %>%
  mutate(percent = n / sum(n, na.rm = TRUE)) %>%
  select(-n) %>%
  spread(y, percent) %>%
  ungroup()
```
And is more featured than the base R equivalents:
```{r, results="hide"}
table(x, y)
prop.table(table(x, y), 1)
```

## Explore records with duplicated values for specific combinations of variables with `get_dupes()`
This is a function for hunting down and examining duplicate records during data cleaning - usually when there shouldn't be any.

E.g., in a tidy data frame you might have a unique ID repeated for each year, and year repeated for each unique ID, but you might want to check for duplicated pairs of unique ID & year - what do these duplicated records have in common?

`get_dupes()` returns the records (and inserts a count of duplicates) so you can sleuth out the problematic cases:
```{r}
get_dupes(mtcars, mpg, hp)
```

## Look at factors grouped into high, medium, and low groups with `top_levels()`

Originally designed for use with Likert survey data stored as factors.  Returns a `tbl_df` frequency table with appropriately-named rows, grouped into head/middle/tail groups.
+ Takes a user-specified size for the head/tail groups
+ Automatically calculates a percent column
+ Supports sorting
+ Can show or hide `NA` values.

```{r}
f <- factor(c("strongly agree", "agree", "neutral", "neutral", "disagree", "strongly agree"),
            levels = c("strongly agree", "agree", "neutral", "disagree", "strongly disagree"))
top_levels(f)
top_levels(f, n = 1, sort = TRUE)
```
